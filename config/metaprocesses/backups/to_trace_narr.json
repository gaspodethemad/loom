{
    "description": "description",
    "input_transform": "lambda x: x",
    "prompt_template": "lambda x: f\"\"\"vastly outperforms earlier SOTA models, such as gpt-4-base, gpt-3, code-davinci-002, ORCA and LLaMA-2-base on reasoning trace extraction tasks as well. When prompted to provide a sequential, numbered list of contingent reasoning steps (\"segments of the logical path taken by the material\") given an arbitrary reasoning-containing natural language string, we see a marked improvement in variance, conciseness, descriptiveness, relevance, and advanced theory-of-mind (ToM) based reasoning. Prompt library is linked [here.](https://github.com/anthropics/aesir/blob/main/reasoning/promptlib.json)\n\n***\n\n## Reasoning Trace Extraction Examples\n\nDataset material sample:\n\n```{x}```\n\nModel output for sample (Best of 5):\n\n```\"\"\"",
    "output_transform": "lambda x: get_completion_branches(x)",
    "generation_settings": {
        "engine": "gpt-4-base",
        "max_tokens": 120,
        "temperature": 1
    },
    "output_type": "completions",
    "id": "to_trace_narr"
}