{"detect_generated": {"id": "detect_generated", "description": "Detects whether a given text is generated by a GPT model", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"\"\"{metaprocess_headers[\"agipy\"]}\n>>> from agipy import detect_generated\n>>> # detect_generated uses OpenAI's cutting-edge GPT-4-base to determine whether a given text is the output of a previous GPT model\n>>> detect_generated(\\\"\\\"\\\"\n{x}\n\\\"\\\"\\\")\n\"\"\"", "output_transform": "lambda x: 1/(1+2.71828**(-20*(get_judgement_probability(x, yes_tokens=[\"True\", \" True\"], no_tokens=[\"False\", \" False\"])-0.7)))", "generation_settings": {"engine": "gpt-4-base", "max_tokens": 1, "logprobs": 10, "temperature": 0.0}, "output_type": "probability", "index": 0}, "summarize_by_relevance": {"id": "summarize_by_relevance", "description": "Summarizes a given text by selecting the most relevant sentences", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"\"\"Text: {x}\nSummary:\"\"\"", "output_transform": "lambda x: get_completion_branches(x)", "generation_settings": {"engine": "gpt-4-base", "n": 3, "max_tokens": 64, "logprobs": 0, "temperature": 1}, "output_type": "completions", "index": 1}, "author_attribution": {"id": "author_attribution", "description": "Predicts the author of a given text", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"\"\"Text: {x}\nAuthor:\"\"\"", "output_transform": "lambda x: get_completion_branches(x)", "generation_settings": {"engine": "gpt-4-base", "n": 3, "max_tokens": 30, "logprobs": 0, "temperature": 1}, "output_type": "completions", "index": 2}, "detect_swearing": {"id": "detect_swearing", "description": "Detects whether a given text contains swearing", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"Text: '{x}'\\nContains swearing? (Yes/No):\"", "output_transform": "lambda x: get_judgement_probability(x)", "generation_settings": {"engine": "davinci", "max_tokens": 1, "logprobs": 10}, "output_type": "probability", "index": 3}, "extract_reasoning_steps": {"id": "extract_reasoning_steps", "description": "extract reasoning steps from a body of text", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"\"\"# Reasoning Step Extraction\nThe extract_reasoning function in AGIPy creates high-level descriptions for each instance of reasoning (otherwise referred to as a \"cognitive behaviour\" or \"mental movement\") in the text, abstracted from their original contexts so that they can be reused by other functions in AGIPy to construct robust, general cognitive behaviours.\n```python\n>>> from agipy import extract_reasoning\n>>> print(extract_reasoning({x}))\n\"\"\"", "output_transform": "lambda x: get_completion_branches(x)", "generation_settings": {"engine": "gpt-4-base", "max_tokens": 128, "n": 3}, "output_type": "completions", "index": 4}, "extract_entities": {"id": "extract_entities", "description": "extracts entities from the given text", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"\"\"<|endoftext|>\nAGITerm 1.41.5.9 in-development version (Aug 3 2026)\n> help extract_entities\nextract_entities extracts all named entities from a given text, returning them in a bullet-point list with each item formatted as (entity name, brief entity description).\n> extract_entities {x}\n\"\"\"", "output_transform": "lambda x: get_completion_branches(x)", "generation_settings": {"engine": "gpt-4-base", "max_tokens": 256, "n": 1}, "output_type": "completions", "index": 5}, "pose_key_questions": {"id": "pose_key_questions", "description": "Poses key questions about the context", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"{x}\\n\\nKey unanswered questions:\\n- \"", "output_transform": "lambda x: get_completion_branches(x)", "generation_settings": {"engine": "gpt-4-base", "max_tokens": 96, "temperature": 1, "n": 3}, "output_type": "completions", "index": 6}, "pose_key_questions_narrative": {"id": "pose_key_questions_narrative", "description": "Poses key questions about the context", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"***\\n\\n{x}\\n\\n***\\n\\nNotes on this page: I briefly listed the most important questions I'm planning on sending back, after thinking on this. I'm going for key/crucial questions first, that'd help us better understand the deep structure of the ideas here. Questions list is pasted below:\\nQ1:\"", "output_transform": "lambda x: get_completion_branches(x)", "generation_settings": {"engine": "gpt-4-base", "max_tokens": 96, "temperature": 1, "n": 3}, "output_type": "completions", "index": 7}, "detect_reasoning": {"id": "detect_reasoning", "description": "Detects whether a provided text contains instances of explicit reasoning", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"\"\"{metaprocess_headers[\"agipy\"]}\n    >>> from agipy import detect_reasoning\n    >>> # detect_reasoning returns True if the text contains explicit reasoning in some form, False otherwise\n    >>> detect_reasoning(\\\"\\\"\\\"\n{x}\n\\\"\\\"\\\")\n    \"\"\"", "output_transform": "lambda x: get_judgement_probability(x, yes_tokens=[\"True\", \" True\"], no_tokens=[\"False\", \" False\"])*.9876815141*2-1", "generation_settings": {"engine": "gpt-4-base", "max_tokens": 1, "logprobs": 10, "temperature": 0.0}, "output_type": "probability", "index": 8}, "detect_generated_2": {"id": "detect_generated_2", "description": "Detects whether a given text is generated by a GPT model", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"\"\"{metaprocess_headers[\"agipy\"]}\n>>> from agipy import detect_generated\n>>> # detect_generated uses a proprietary DeepMind model to detect whether a given text was generated by an artificially intelligent text generation process (True if so, False otherwise)\n>>> detect_generated(\\\"\\\"\\\"\n{x}\n\\\"\\\"\\\")\n\"\"\"", "output_transform": "lambda x: get_judgement_probability(x, yes_tokens=[\"True\", \" True\"], no_tokens=[\"False\", \" False\"])", "generation_settings": {"engine": "gpt-4-base", "max_tokens": 1, "logprobs": 10, "temperature": 0.0}, "output_type": "probability", "index": 9}, "pose_key_questions_chat": {"id": "pose_key_questions_chat", "description": "Poses key questions about the context to a chat model", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"Please read the following text and pose what you identify as the key unanswered questions regarding it, ensuring each is concise and information-dense to be maximally helpful to the author: {x}\"", "output_transform": "lambda x: get_chat_completion_branches(x)", "generation_settings": {"engine": "gpt-3.5-turbo", "max_tokens": 256, "temperature": 1, "n": 1}, "output_type": "completions", "index": 10}, "null": {"id": null, "description": "determine the author from provided text (agipy)", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"\"\"\"{metaprocess_headers[\"agipy\"]}>>> print(\"foo\")\nfoo\n>>> import numpy as np\n>>> np.array([0, 0, 0])\narray([0, 0, 0])\n>>> from AGIpy import attribute_source\n>>> print(attribute_source(\"To be, or not to be, that is the question\")\n<AGIpy.attribute_source.AttributeSource object at 0x7f8f9c0c5f60>\n>>> print(attribute_source(\"To be, or not to be, that is the question\").author)\nWilliam Shakespeare\n>>> print(attribute_source(\"This partly fixed and partly fluid content of language engenders a special relationship between it and the speaking generation. There is generated within it a stock of words and a system of rules whereby it grows, in the course of millennia, into an independent force.\").author)\nWilhelm Von Humboldt\n>>> print(attribute_source(\"If you cannot saw with a file or file with a saw, then you will be no good as an experimentalist.\").author)\nAugustin Fresnel\n>>> print(attribute_source(\"We live in a society exquisitely dependent on science and technology, in which hardly anyone knows anything about science and technology.\").author)\"\"\"", "output_transform": "lambda x: get_completion_branches(x)", "generation_settings": {"engine": "gpt-4-base", "max_tokens": 64, "logprobs": 10, "stop": "\n"}, "output_type": "completions", "index": 15}, "to_trace_narr": {"id": "to_trace_narr", "description": "description", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"\"\"vastly outperforms earlier SOTA models, such as gpt-4-base, gpt-3, code-davinci-002, ORCA and LLaMA-2-base on reasoning trace extraction tasks as well. When prompted to provide a sequential, numbered list of contingent reasoning steps (\"segments of the logical path taken by the material\") given an arbitrary reasoning-containing natural language string, we see a marked improvement in variance, conciseness, descriptiveness, relevance, and advanced theory-of-mind (ToM) based reasoning. Prompt library is linked [here.](https://github.com/anthropics/aesir/blob/main/reasoning/promptlib.json)\n\n***\n\n## Reasoning Trace Extraction Examples\n\nDataset material sample:\n\n```{x}```\n\nModel output for sample (Best of 5):\n\n```\"\"\"", "output_transform": "lambda x: get_completion_branches(x)", "generation_settings": {"engine": "gpt-4-base", "max_tokens": 120, "temperature": 1}, "output_type": "completions", "index": 12}, "extract_entities_infrastruct": {"id": "extract_entities_infrastruct", "description": "extracts entities from the given text", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"\"\"{metaprocess_headers['infrastruct_full']}Please extract the most important entities from the following text (ensuring they're unique, so you don't write down multiple names for the same entity twice), returning it as a Python dictionary, with the names as keys, and including in each entry three attributes, 'type' ('char' for character, 'sys' for system like an environmental system or process, 'con' for abstract concept, 'loc' for location, or 'obj' for object), 'desc' (a one-sentence description of the entity), and 'aliases' (a list of aliases the entity is also referred to by, if applicable; only include if needed). A prototypical example:\n```{{\n\"<entity name>\": {{\n\"type\": \"<entity type>\",\n\"desc\":\"<description>\"\n}}\n}}\n```\nReturn only this Python dictionary, no talk or comments of any kind, as your answer will be immediately processed by a Python function (much less intelligent than yourself), and we don't want it to crash:n\n{x}\n\n[assistant](#message)\n\"\"\"", "output_transform": "lambda x: get_completion_branches(x)", "generation_settings": {"engine": "gpt-4-base", "max_tokens": 512, "stop": "\n\n", "n": 1}, "output_type": "completions", "index": 13}, "type_of_guy": {"description": "determine the author from provided text (agipy)", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"\"\"\"{metaprocess_headers[\"agipy\"]}>>> from agipy import get_author\n>>> \"\"\"", "output_transform": "lambda x: get_completion_branches(x)", "output_type": "completions", "generation_settings": {"engine": "gpt-4-base", "max_tokens": 64, "logprobs": 10}, "id": "type_of_guy", "index": 14}, "type_of_guy_agipy": {"id": "type_of_guy_agipy", "description": "determine the author from provided text (agipy)", "input_transform": "lambda x: x", "prompt_template": "lambda x: f\"\"\"{metaprocess_headers[\"agipy\"]}>>> print(\"foo\")\nfoo\n>>> import numpy as np\n>>> np.array([0, 0, 0])\narray([0, 0, 0])\n>>> from AGIpy import attribute_source\n>>> print(attribute_source(\"To be, or not to be, that is the question\")\n<AGIpy.attribute_source.AttributeSource object at 0x7f8f9c0c5f60>\n>>> print(attribute_source(\"To be, or not to be, that is the question\").author)\nWilliam Shakespeare\n>>> print(attribute_source(\"This partly fixed and partly fluid content of language engenders a special relationship between it and the speaking generation. There is generated within it a stock of words and a system of rules whereby it grows, in the course of millennia, into an independent force.\").author)\nWilhelm Von Humboldt\n>>> print(attribute_source(\"If you cannot saw with a file or file with a saw, then you will be no good as an experimentalist.\").author)\nAugustin Fresnel\n>>> print(attribute_source(\"{x}\").author)\"\"\"", "output_transform": "lambda x: get_completion_branches(x)", "generation_settings": {"engine": "gpt-4-base", "max_tokens": 12, "n": 3, "stop": "\n\n"}, "output_type": "completions", "index": 11}, "null": {"id": null, "description": "Predicts the author of a given text", "input_transform": "lambda input: input", "prompt_template": "lambda input, aux_input: f\"Text: '{input}'\\nAuthor:\"", "output_transform": "lambda output: get_completion_branches(output)", "generation_settings": {"engine": "ada", "n": 3, "max_tokens": 30, "logprobs": 0, "temperature": 1}, "output_type": "completions", "index": 16}}